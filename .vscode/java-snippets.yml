snippet java-mem-dependencies:
  name: "java-mem-dependencies"
  prefix: "java-01-mem-dependencies"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j</artifactId>
      <version>${langchain4j.version}</version>
    </dependency>

    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mistral-ai</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet java-mem-interface:
  name: "java-mem-interface"
  prefix: "java-02-mem-interface"
  scope: "java"
  body: |
    interface Assistant {
      TokenStream chat(String message);
    }

snippet java-mem-model:
  name: "java-mem-model"
  prefix: "java-03-mem-model"
  scope: "java"
  body: |
    MistralAiStreamingChatModel streamingChatModel = MistralAiStreamingChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(
                    System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .maxTokens(512)
            .build();

snippet java-mem-memory:
  name: "java-mem-memory"
  prefix: "java-04-mem-memory"
  scope: "java"
  body: |
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-mem-assistant:
  name: "java-mem-assistant"
  prefix: "java-05-mem-assistant"
  scope: "java"
  body: |
    Assistant assistant = AiServices.builder(Assistant.class)
        .streamingChatModel(streamingChatModel)
        .chatMemory(chatMemory)
    .build();

snippet java-mem-prompt:
  name: "java-mem-prompt"
  prefix: "java-06-mem-prompt"
  scope: "java"
  body: |
    _LOG.info("ðŸ’¬: My name is StÃ©phane.\n");
    TokenStream tokenStream = assistant.chat("My name is StÃ©phane.");
    _LOG.info("ðŸ¤–: ");
    tokenStream
        .onPartialResponse(_LOG::info)
        .onCompleteResponse(token -> {
          _LOG.info("\nðŸ’¬: Do you remember what is my name?\n");
          _LOG.info("ðŸ¤–: ");
          assistant.chat("Do you remember what is my name?")
              .onPartialResponse(_LOG::info)
              .onError(Throwable::printStackTrace).start();
        })
        .onError(Throwable::printStackTrace).start();

snippet java-rag-dependency:
  name: "java-rag-dependency"
  prefix: "java-07-rag-dependency"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-ovh-ai</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet java-rag-chunk:
  name: "java-rag-chunk"
  prefix: "java-08-rag-chunk"
  scope: "java"
  body: |
    // Load the document and split it into chunks
    DocumentParser documentParser = new TextDocumentParser();
    Document document = loadDocument(
      RAGStreamingChatbot.class.getResource("/rag-files/content.txt").getFile(),
      documentParser
    );
    DocumentSplitter splitter = DocumentSplitters.recursive(400, 0);

    List<TextSegment> segments = splitter.split(document);

snippet java-rag-embed:
  name: "java-rag-embed"
  prefix: "java-09-rag-embed"
  scope: "java"
  body: |
    // Do the embeddings and store them in an embedding store
    EmbeddingModel embeddingModel = OvhAiEmbeddingModel.builder()
            .apiKey(OVHCLOUD_API_KEY)
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_EMBEDDING_MODEL_URL"))
            .build();
    List<Embedding> embeddings = embeddingModel.embedAll(segments).content();

snippet java-rag-store:
  name: "java-rag-store"
  prefix: "java-10-rag-store"
  scope: "java"
  body: |
    EmbeddingStore<TextSegment> embeddingStore = new InMemoryEmbeddingStore<>();
    embeddingStore.addAll(embeddings, segments);
    ContentRetriever contentRetriever = EmbeddingStoreContentRetriever.builder()
      .embeddingStore(embeddingStore)
      .embeddingModel(embeddingModel)
      .maxResults(5)
      .minScore(0.3)
    .build();

snippet java-rag-model:
  name: "java-rag-model"
  prefix: "java-11-rag-model"
  scope: "java"
  body: |
    MistralAiStreamingChatModel streamingChatModel = MistralAiStreamingChatModel.builder()
            .apiKey(OVHCLOUD_API_KEY)
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .baseUrl(
                    System.getenv("OVH_AI_ENDPOINTS_MODEL_URL")
            )
            .maxTokens(512)
            .build();

snippet java-rag-assistant:
  name: "java-rag-assistant"
  prefix: "java-12-rag-assistant"
  scope: "java"
  body: |
    Assistant assistant = AiServices
      .builder(Assistant.class)
      .streamingChatModel(streamingChatModel)
      .contentRetriever(contentRetriever)
      .build();

snippet java-rag-prompt:
  name: "java-rag-prompt"
  prefix: "java-13-rag-prompt"
  scope: "java"
  body: |
    _LOG.info("\nðŸ’¬: What is AI Endpoints?\n");

    TokenStream tokenStream = assistant.chat("Can you explain me what is AI Endpoints?");
    _LOG.info("ðŸ¤–: ");
    tokenStream
        .onPartialResponse(_LOG::info)
        .onError(Throwable::printStackTrace)
        .start();

snippet java-sentiment-dependency:
  name: "java-sentiment-dependency"
  prefix: "java-14-sentiment-dependency"
  scope: "xml"
  body: |
    <dependency>
      <groupId>fr.wilda.ai</groupId>
      <artifactId>java-ai-toolkit</artifactId>
      <version>1.0.0</version>
    </dependency>

snippet java-sentiment-http:
  name: "java-sentiment-http"
  prefix: "java-15-sentiment-http"
  scope: "java"
  body: |
    // Create a HTTP client
    HttpClient client = HttpClient.newHttpClient();

snippet java-sentiment-request:
  name: "java-sentiment-request"
  prefix: "java-16-sentiment-request"
  scope: "java"
  body: |
    // Create a request builder
    HttpRequest request = HttpRequest.newBuilder()
            .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SENTIMENT_MODEL_URL")))
            .POST(HttpRequest.BodyPublishers.ofString("I love Java"))
            .header("Content-Type", "application/json")
            .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .build();

snippet java-sentiment-response:
  name: "java-sentiment-response"
  prefix: "java-17-sentiment-response"
  scope: "java"
  body: |
    // Send the request and get the response
    HttpResponse<String> response = client.send(request, HttpResponse.BodyHandlers.ofString());

snippet java-sentiment-mapper:
  name: "java-sentiment-mapper"
  prefix: "java-18-sentiment-mapper"
  scope: "java"
  body: |
    // Deserialize the response to a Java object using Jackson
    ObjectMapper objectMapper = new ObjectMapper();
    Emotion responseObject = objectMapper.readValue(response.body(), Emotion.class);

snippet java-sentiment-display:
  name: "java-sentiment-display"
  prefix: "java-19-sentiment-display"
  scope: "java"
  body: |
    // Print the response status code and body
    _LOG.debug("Status code: " + response.statusCode());
    _LOG.debug("Response body: " + response.body());
    _LOG.info("Emotion: " + responseObject.toEmoji());

snippet java-20:
  name: "java-20"
  prefix: "java-20"
  scope: "java"
  body: |
    @Tool("""
                Tool to create an image with Stable Diffusion XL given a prompt and a negative prompt.
                """)
    void generateImage(@P("Prompt that explains the image") String prompt, @P("Negative prompt that explains what the image must not contains") String negativePrompt) throws IOException, InterruptedException {
        _LOG.info("Prompt: {}", prompt);
        _LOG.info("Negative prompt: {}", negativePrompt);

        // java-21
    
        // java-22
    }

snippet java-21:
  name: "java-21"
  prefix: "java-21"
  scope: "java"
  body: |
    HttpRequest httpRequest = HttpRequest.newBuilder()
    .uri(URI.create(System.getenv("OVH_AI_ENDPOINTS_SD_URL")))
    .POST(HttpRequest.BodyPublishers.ofString("""
            {"prompt": "%s",
             "negative_prompt": "%s"}
            """.formatted(prompt, negativePrompt)))
    .header("accept", "application/octet-stream")
    .header("Content-Type", "application/json")
    .header("Authorization", "Bearer " + System.getenv("OVH_AI_ENDPOINTS_SDXL_ACCESS_TOKEN"))
    .build();

snippet java-22:
  name: "java-22"
  prefix: "java-22"
  scope: "java"
  body: |
    HttpResponse<byte[]> response = HttpClient.newHttpClient()
            .send(httpRequest, HttpResponse.BodyHandlers.ofByteArray());
    
    _LOG.debug("SDXL status code: {}", response.statusCode());
    Files.write(Path.of("generated-image.jpeg"), response.body());

snippet java-23:
  name: "java-23"
  prefix: "java-23"
  scope: "java"
  body: |
    @SystemMessage("""
            Your are an expert of using the Stable Diffusion XL model.
            The user explains in natural language what kind of image he wants.
            You must do the following steps:
              - Understand the user's request.
              - Generate the two kinds of prompts for stable diffusion: the prompt and the negative prompt
              - the prompts must be in english and detailed and optimized for the Stable Diffusion XL model. 
              - once and only once you have this two prompts call the tool with the two prompts.
            If asked about to create an image, you MUST call the `generateImage` function.
            """)
    @UserMessage("Create an image with stable diffusion XL following this description: {{userMessage}}")
    String chat(@V("userMessage") String userMessage);

snippet java-24:
  name: "java-24"
  prefix: "java-24"
  scope: "java"
  body: |
    // Main chatbot configuration, choose on of the available models on the AI Endpoints catalog (https://endpoints.ai.cloud.ovh.net/catalog)
    ChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            // To have more deterministic outputs, set temperature to 0.
            .temperature(0.0)
            .build();

snippet java-25:
  name: "java-25"
  prefix: "java-25"
  scope: "java"
  body: |
    // Add memory to fine tune the SDXL prompt.
    ChatMemory chatMemory = MessageWindowChatMemory.withMaxMessages(10);

snippet java-26:
  name: "java-26"
  prefix: "java-26"
  scope: "java"
  body: |
    // Build the chatbot thanks to LangChain4J AI Services mode (see https://docs.langchain4j.dev/tutorials/ai-services)
    ChatBot chatBot = AiServices.builder(ChatBot.class)
            .chatModel(chatModel)
            .tools(new ImageGenTools())
            .chatMemory(chatMemory)
            .build();

snippet java-27:
  name: "java-27"
  prefix: "java-27"
  scope: "java"
  body: |
    // Start the conversation loop (enter "exit" to quit)
    String userInput = "";
    Scanner scanner = new Scanner(System.in);
    while (true) {
        _LOG.info("Enter your message: ");
        userInput = scanner.nextLine();
        if (userInput.equalsIgnoreCase("exit")) break;
        _LOG.info("Response: " + chatBot.chat(userInput));
    }
    scanner.close();

snippet java-28-MCP:
  name: "java-28-MCP"
  prefix: "java-28-MCP"
  scope: "xml"
  body: |
    <dependency>
      <groupId>dev.langchain4j</groupId>
      <artifactId>langchain4j-mcp</artifactId>
      <version>${langchain4j.provider.version}</version>
    </dependency>

snippet java-29:
  name: "java-29"
  prefix: "java-29"
  scope: "java"
  body: |
    // Simple chatbot definition with AI Services from LangChain4J
    public interface Bot {
        String chat(String prompt);
    }

snippet java-30:
  name: "java-30"
  prefix: "java-30"
  scope: "java"
  body: |
    // Mistral model from OVHcloud AI Endpoints
    ChatModel chatModel = MistralAiChatModel.builder()
            .apiKey(System.getenv("OVH_AI_ENDPOINTS_ACCESS_TOKEN"))
            .baseUrl(System.getenv("OVH_AI_ENDPOINTS_MODEL_URL"))
            .modelName(System.getenv("OVH_AI_ENDPOINTS_MODEL_NAME"))
            .logRequests(false)
            .logResponses(false)
            .build();

snippet java-31:
  name: "java-31"
  prefix: "java-31"
  scope: "java"
  body: |
    // Configure the MCP server to use
    McpTransport transport = new HttpMcpTransport.Builder()
            // https://xxxx/mcp/sse
            .sseUrl(System.getenv("MCP_SERVER_URL"))
            .logRequests(false)
            .logResponses(false)
            .build();

snippet java-32:
  name: "java-32"
  prefix: "java-32"
  scope: "java"
  body: |
    // Create the MCP client for the given MCP server
    McpClient mcpClient = new DefaultMcpClient.Builder()
            .transport(transport)
            .build();

snippet java-33:
  name: "java-33"
  prefix: "java-33"
  scope: "java"
  body: |
    // Configure the tools list for the LLM
    McpToolProvider toolProvider = McpToolProvider.builder()
            .mcpClients(mcpClient)
            .build();

snippet java-34:
  name: "java-34"
  prefix: "java-34"
  scope: "java"
  body: |
    // Create the chatbot with the given LLM and tools list
    Bot bot = AiServices.builder(Bot.class)
            .chatModel(chatModel)
            .toolProvider(toolProvider)
            .build();

snippet java-35:
  name: "java-35"
  prefix: "java-35"
  scope: "java"
  body: |
    // Play with the chatbot ðŸ¤–
    String response = bot.chat("Can I have some details about my OVHcloud account?");
    _LOG.info("RESPONSE: {}",  response);

snippet quarkus-ai-langchain4j:
  name: "quarkus-ai-langchain4j"
  prefix: "quarkus-01-ai-langchain4j"
  body: |
    quarkus.langchain4j.mistralai.base-url=\${OVH_AI_ENDPOINTS_MODEL_URL}
    quarkus.langchain4j.mistralai.api-key=\${OVH_AI_ENDPOINTS_ACCESS_TOKEN}
    quarkus.langchain4j.mistralai.chat-model.max-tokens=512
    quarkus.langchain4j.mistralai.chat-model.model-name=\${OVH_AI_ENDPOINTS_MODEL_NAME}
    quarkus.langchain4j.mistralai.log-requests=false
    quarkus.langchain4j.mistralai.log-responses=true
    quarkus.langchain4j.mistralai.chat-model.temperature=0.2

snippet quarkus-ai-ai-service:
  name: "quarkus-ai-ai-service"
  prefix: "quarkus-02-ai-ai-service"
  scope: "java"
  body: |
    @SessionScoped
    @RegisterAiService(chatMemoryProviderSupplier = RegisterAiService.NoChatMemoryProviderSupplier.class)

snippet quarkus-ai-prompt:
  name: "quarkus-ai-prompt"
  prefix: "quarkus-03-ai-prompt"
  scope: "java"
  body: |
    @SystemMessage("You are a virtual assistant and your name is JARVIS.")
    @UserMessage("Answer as best possible to the following question: {question}. The answer must be in a style of a virtual assistant.")
    Multi<String> askAQuestion(String question);

snippet quarkus-websocket:
  name: "quarkus-websocket"
  prefix: "quarkus-04-websocket"
  scope: "java"
  body: |
    @WebSocket(path = "/jarvis")

snippet quarkus-ws-inject-service:
  name: "quarkus-ws-inject-service"
  prefix: "quarkus-05-ws-inject-service"
  scope: "java"
  body: |
    @Inject
    AIEndpointService aiEndpointService;

snippet quarkus-ws-onMessage:
  name: "quarkus-ws-onMessage"
  prefix: "quarkus-06-ws-onMessage"
  scope: "java"
  body: |
    return aiEndpointService.askAQuestion(message)
        .onItem()
        .call(i -> Uni.createFrom()
            .nullItem()
            .onItem()
            .delayIt()
            .by(Duration.ofMillis(200)));

snippet quarkus-ws-cors:
  name: "quarkus-ws-cors"
  prefix: "quarkus-07-ws-cors"
  body: |
    # Disable CORS
    quarkus.http.cors=true
    %dev.quarkus.http.cors.origins=/.*/

snippet quarkus-08:
  name: "quarkus-08"
  prefix: "quarkus-08"
  scope: "xml"
  body: |
    <dependency>
        <groupId>io.quarkiverse.mcp</groupId>
        <artifactId>quarkus-mcp-server-sse</artifactId>
        <version>1.2.1</version>
    </dependency>

snippet quarkus-09:
  name: "quarkus-09"
  prefix: "quarkus-09"
  body: |
    # OVHcloud parameter
    ovhcloud.consumer=${OVH_CONSUMER_KEY}
    ovhcloud.application=${OVH_APPLICATION_KEY}
    ovhcloud.projectId=${OVH_CLOUD_PROJECT_SERVICE}
    
    # RestClient parameter
    quarkus.rest-client."com.ovhcloud.sdk.service.OVHcloudMe".url=https://eu.api.ovh.com/

snippet quarkus-10:
  name: "quarkus-10"
  prefix: "quarkus-10"
  scope: "java"
  body: |
    @Tool(description = "Tool to manage the OVHcloud public cloud user.")
    ToolResponse getUserDetails() {
        Long ovhTimestamp = System.currentTimeMillis() / 1000;
        return ToolResponse.success(
                new TextContent(ovhcloudMe.getMe(OVHcloudSignatureHelper.signature("me", ovhTimestamp),
                        Long.toString(ovhTimestamp)).toString()));
    }
